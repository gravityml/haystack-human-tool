{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Prerequisites for colab. skip if you are running this notebook locally and are installing dependencies manually.\n",
    "For more haystack installation options see https://docs.haystack.deepset.ai/docs/installation#installing-haystack-core"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%bash is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install datasets\n",
    "pip install farm-haystack[colab]\n",
    "pip install --upgrade haystack-memory\n",
    "pip install --upgrade haystack-human-tool\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Activate logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:01:49.595689Z",
     "start_time": "2023-04-16T15:01:47.132401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key:········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:05:46.436505Z",
     "start_time": "2023-04-16T15:05:46.148972Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/haystack2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/rjtannous/github/vue-test/haystack/haystack/document_stores/search_engine.py:169: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  return self.client.indices.exists(index_name, headers=headers)\n",
      "/Users/rjtannous/github/vue-test/haystack/haystack/document_stores/elasticsearch.py:499: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  indices = self.client.indices.get(index_name, headers=headers)\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore, ElasticsearchDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(\n",
    "    similarity=\"cosine\",\n",
    "    index=\"document\",\n",
    "    embedding_field=\"embedding\",\n",
    "    embedding_dim=1536\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:17:28.383112Z",
     "start_time": "2023-04-06T18:17:27.131964Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load seven wonders dataset from datasets and write documents into elasticsearch document store\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "document_store.write_documents(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:05:54.785119Z",
     "start_time": "2023-04-16T15:05:54.206495Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever, OpenAIAnswerGenerator\n",
    "\n",
    "# define retriever model , update dense vector embeddings on document store index and define generator model\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    api_key=openai_api_key,\n",
    "    max_seq_len=1024,\n",
    "    top_k=4,\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:06:01.105372Z",
     "start_time": "2023-04-16T15:06:01.092416Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "\n",
    "QA_promptnode = PromptTemplate(\n",
    "            name=\"zero-shot-QA\", \n",
    "            prompt_text=\"You are a helpful and knowledgeable agent. Only Answer if the {documents} contain the answer. If the user question is not related to the provided {documents}, say I don't have an answer\\n\"\n",
    "            \"Question: {query}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:06:05.001834Z",
     "start_time": "2023-04-16T15:06:04.485291Z"
    }
   },
   "outputs": [],
   "source": [
    "QA_builder = PromptNode(model_name_or_path=\"text-davinci-003\", api_key=openai_api_key, default_prompt_template=QA_promptnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T15:06:09.890144Z",
     "start_time": "2023-04-16T15:06:09.886961Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=QA_builder, name=\"Generator\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T16:47:47.949012Z",
     "start_time": "2023-04-16T16:47:47.404556Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack.agents import Agent, Tool\n",
    "from haystack.nodes import PromptNode\n",
    "from haystack_human_tool.prompt_templates import agent_template\n",
    "\n",
    "# create agent prompt node and define our Agent \"memory_agent\"\n",
    "prompt_node = PromptNode(model_name_or_path=\"text-davinci-003\", api_key=openai_api_key, max_length=512,\n",
    "                         stop_words=[\"Observation:\"])\n",
    "memory_agent = Agent(prompt_node=prompt_node, prompt_template=agent_template)\n",
    "# Define the first tool: A document store QA tool based on the pipeline\n",
    "search_tool = Tool(name=\"DocumentStore_QA\",\n",
    "                   pipeline_or_node=pipe,\n",
    "                   description=\"Access this tool to find information needed to answer questions.\",\n",
    "                   output_variable=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T16:47:49.763968Z",
     "start_time": "2023-04-16T16:47:49.762575Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack_memory.memory import MemoryRecallNode\n",
    "\n",
    "sensory_memory = []\n",
    "working_memory = []\n",
    "memory_node = MemoryRecallNode(memory=working_memory)\n",
    "memory_tool = Tool(name=\"Memory\",\n",
    "                   pipeline_or_node=memory_node,\n",
    "                   description=\"Your memory. Always access this tool first to remember what you have learned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T16:47:51.966075Z",
     "start_time": "2023-04-16T16:47:51.962217Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack_human_tool.humaninloop import HumanInLoopNode\n",
    "\n",
    "human_node = HumanInLoopNode(sensory_memory=sensory_memory)\n",
    "human_tool = Tool(name=\"Human\",\n",
    "                  pipeline_or_node=human_node,\n",
    "                  description=\"Access this tool to refine your query. Ask the human to rephrase the question or to explain what or who the human is referring to. Use the human's answer to rephrase your query for use as input to other tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T16:47:52.524841Z",
     "start_time": "2023-04-16T16:47:52.521882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the human, memory and search tools to the agent\n",
    "memory_agent.add_tool(memory_tool)\n",
    "memory_agent.add_tool(human_tool)\n",
    "memory_agent.add_tool(search_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T16:48:17.249701Z",
     "start_time": "2023-04-16T16:47:53.189578Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack_memory.utils import MemoryUtils\n",
    "\n",
    "# Initialize the agent with memory\n",
    "memory_utils = MemoryUtils(working_memory=working_memory, sensory_memory=sensory_memory, agent=memory_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Assistant started with {'query': 'where did his golden sandals rest?', 'params': None}\n",
      "\u001B[32m find\u001B[0m\u001B[32m out\u001B[0m\u001B[32m what\u001B[0m\u001B[32m the\u001B[0m\u001B[32m person\u001B[0m\u001B[32m is\u001B[0m\u001B[32m talking\u001B[0m\u001B[32m about\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Human\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Could\u001B[0m\u001B[32m you\u001B[0m\u001B[32m please\u001B[0m\u001B[32m explain\u001B[0m\u001B[32m what\u001B[0m\u001B[32m you\u001B[0m\u001B[32m mean\u001B[0m\u001B[32m by\u001B[0m\u001B[32m '\u001B[0m\u001B[32mgold\u001B[0m\u001B[32men\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m'?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0mCould you please explain what you mean by 'golden sandals'?\n",
      "zeus' golden sandals\n",
      "Observation: \u001B[33mzeus' golden sandals\u001B[0m\n",
      "Thought: \u001B[32m Let\u001B[0m\u001B[32m's\u001B[0m\u001B[32m use\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m to\u001B[0m\u001B[32m see\u001B[0m\u001B[32m if\u001B[0m\u001B[32m we\u001B[0m\u001B[32m can\u001B[0m\u001B[32m find\u001B[0m\u001B[32m any\u001B[0m\u001B[32m information\u001B[0m\u001B[32m about\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m \u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Where\u001B[0m\u001B[32m did\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m rest\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "/Users/rjtannous/github/vue-test/haystack/haystack/document_stores/elasticsearch.py:376: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  result = self.client.search(index=index, body=body, request_timeout=300, headers=headers)[\"hits\"][\"hits\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: \u001B[33mZeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief. The passage underneath the throne was restricted by painted screens.\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m observation\u001B[0m\u001B[32m provides\u001B[0m\u001B[32m us\u001B[0m\u001B[32m with\u001B[0m\u001B[32m the\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m to\u001B[0m\u001B[32m our\u001B[0m\u001B[32m question\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mFinal\u001B[0m\u001B[32m Answer\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m rested\u001B[0m\u001B[32m upon\u001B[0m\u001B[32m a\u001B[0m\u001B[32m foot\u001B[0m\u001B[32mst\u001B[0m\u001B[32mool\u001B[0m\u001B[32m decorated\u001B[0m\u001B[32m with\u001B[0m\u001B[32m an\u001B[0m\u001B[32m Amazon\u001B[0m\u001B[32momach\u001B[0m\u001B[32my\u001B[0m\u001B[32m in\u001B[0m\u001B[32m relief\u001B[0m\u001B[32m.\u001B[0m\u001B[32m The\u001B[0m\u001B[32m passage\u001B[0m\u001B[32m underneath\u001B[0m\u001B[32m the\u001B[0m\u001B[32m throne\u001B[0m\u001B[32m was\u001B[0m\u001B[32m restricted\u001B[0m\u001B[32m by\u001B[0m\u001B[32m painted\u001B[0m\u001B[32m screens\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\u001B[0m"
     ]
    }
   ],
   "source": [
    "#chat with agent example one\n",
    "# enter zeus' golden sandals when prompted by agent\n",
    "result = memory_utils.chat(\"where did his golden sandals rest?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['where did his golden sandals rest?', \"Could you please explain what you mean by 'golden sandals'?\", \"zeus' golden sandals\", \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief. The passage underneath the throne was restricted by painted screens.\"]\n"
     ]
    }
   ],
   "source": [
    "print(working_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Assistant started with {'query': 'which muslim general raided the city of Rhodes?', 'params': None}\n",
      "\u001B[32m access\u001B[0m\u001B[32m my\u001B[0m\u001B[32m memory\u001B[0m\u001B[32m to\u001B[0m\u001B[32m see\u001B[0m\u001B[32m if\u001B[0m\u001B[32m I\u001B[0m\u001B[32m can\u001B[0m\u001B[32m remember\u001B[0m\u001B[32m the\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m.\u001B[0m\u001B[32m \u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Memory\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Which\u001B[0m\u001B[32m mus\u001B[0m\u001B[32mlim\u001B[0m\u001B[32m general\u001B[0m\u001B[32m raided\u001B[0m\u001B[32m the\u001B[0m\u001B[32m city\u001B[0m\u001B[32m of\u001B[0m\u001B[32m Rhodes\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0mObservation: \u001B[33m['where did his golden sandals rest?', \"Could you please explain what you mean by 'golden sandals'?\", \"zeus' golden sandals\", \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief. The passage underneath the throne was restricted by painted screens.\"]\u001B[0m\n",
      "Thought: \u001B[32m The\u001B[0m\u001B[32m observation\u001B[0m\u001B[32m I\u001B[0m\u001B[32m received\u001B[0m\u001B[32m from\u001B[0m\u001B[32m the\u001B[0m\u001B[32m memory\u001B[0m\u001B[32m tool\u001B[0m\u001B[32m does\u001B[0m\u001B[32m not\u001B[0m\u001B[32m contain\u001B[0m\u001B[32m the\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m I\u001B[0m\u001B[32m am\u001B[0m\u001B[32m looking\u001B[0m\u001B[32m for\u001B[0m\u001B[32m.\u001B[0m\u001B[32m Let\u001B[0m\u001B[32m's\u001B[0m\u001B[32m try\u001B[0m\u001B[32m to\u001B[0m\u001B[32m refine\u001B[0m\u001B[32m my\u001B[0m\u001B[32m query\u001B[0m\u001B[32m and\u001B[0m\u001B[32m pass\u001B[0m\u001B[32m it\u001B[0m\u001B[32m to\u001B[0m\u001B[32m the\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m tool\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Who\u001B[0m\u001B[32m was\u001B[0m\u001B[32m the\u001B[0m\u001B[32m mus\u001B[0m\u001B[32mlim\u001B[0m\u001B[32m general\u001B[0m\u001B[32m who\u001B[0m\u001B[32m raided\u001B[0m\u001B[32m the\u001B[0m\u001B[32m city\u001B[0m\u001B[32m of\u001B[0m\u001B[32m Rhodes\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "/Users/rjtannous/github/vue-test/haystack/haystack/document_stores/elasticsearch.py:376: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  result = self.client.search(index=index, body=body, request_timeout=300, headers=headers)[\"hits\"][\"hits\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: \u001B[33mMuawiyah I was the Muslim general who raided the city of Rhodes in 653.\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m observation\u001B[0m\u001B[32m contains\u001B[0m\u001B[32m the\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m I\u001B[0m\u001B[32m am\u001B[0m\u001B[32m looking\u001B[0m\u001B[32m for\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mFinal\u001B[0m\u001B[32m Answer\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Mu\u001B[0m\u001B[32maw\u001B[0m\u001B[32miyah\u001B[0m\u001B[32m I\u001B[0m\u001B[32m was\u001B[0m\u001B[32m the\u001B[0m\u001B[32m Muslim\u001B[0m\u001B[32m general\u001B[0m\u001B[32m who\u001B[0m\u001B[32m raided\u001B[0m\u001B[32m the\u001B[0m\u001B[32m city\u001B[0m\u001B[32m of\u001B[0m\u001B[32m Rhodes\u001B[0m\u001B[32m in\u001B[0m\u001B[32m 6\u001B[0m\u001B[32m53\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\u001B[0m"
     ]
    }
   ],
   "source": [
    "result = memory_utils.chat(\"which muslim general raided the city of Rhodes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['where did his golden sandals rest?', \"Could you please explain what you mean by 'golden sandals'?\", \"zeus' golden sandals\", \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief. The passage underneath the throne was restricted by painted screens.\", 'which muslim general raided the city of Rhodes?', 'Muawiyah I was the Muslim general who raided the city of Rhodes in 653.']\n"
     ]
    }
   ],
   "source": [
    "print(working_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
